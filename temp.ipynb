{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.cuda.is_available())\n",
    "print(torch.cuda.device_count())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.models as models\n",
    "from torchvision.models import ResNet50_Weights\n",
    "import math\n",
    "from models.transformer import   \n",
    "\n",
    "# 1. Positional Encoding for 2D spatial information\n",
    "class PositionalEncoding2D(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model, height, width):\n",
    "        super(PositionalEncoding2D, self).__init__()\n",
    "        self.d_model = d_model\n",
    "\n",
    "        # Create a positional encoding matrix\n",
    "        pe = torch.zeros(height, width, d_model)\n",
    "        y_pos = torch.arange(height, dtype=torch.float).unsqueeze(1)  # Shape: (height, 1)\n",
    "        x_pos = torch.arange(width, dtype=torch.float).unsqueeze(0)   # Shape: (1, width)\n",
    "\n",
    "        # Calculate the positional encodings\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * -(math.log(10000.0) / d_model))  # Shape: (d_model/2)\n",
    "\n",
    "        # Apply sine and cosine to even and odd indices\n",
    "        pe[:, :, 0::2] = torch.sin(y_pos * div_term.unsqueeze(0))  # Shape: (height, width, d_model/2)\n",
    "        pe[:, :, 1::2] = torch.cos(x_pos * div_term)              # Shape: (height, width, d_model/2)\n",
    "\n",
    "        # Reshape to (height * width, d_model)\n",
    "        pe = pe.view(height * width, d_model)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Add positional encoding to the input tensor\n",
    "        return x + self.pe[:x.size(1), :].unsqueeze(0) \n",
    "\n",
    "# 2. ResNet-50 Backbone to extract feature maps\n",
    "class ResNetFeatureExtractor(nn.Module):\n",
    "    def __init__(self, d_model):\n",
    "        super(ResNetFeatureExtractor, self).__init__()\n",
    "        resnet = models.resnet50(weights=ResNet50_Weights.IMAGENET1K_V1)\n",
    "        self.backbone = nn.Sequential(*list(resnet.children())[:-2])  # Remove FC layer and AvgPool\n",
    "        self.conv = nn.Conv2d(2048, d_model, kernel_size=1)  # Reduce channels to d_model\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.backbone(x)  # Shape: [B, 2048, H, W]\n",
    "        x = self.conv(x)      # Shape: [B, d_model, H, W]\n",
    "        return x\n",
    "\n",
    "# 3. Transformer Decoder with CTC Head\n",
    "class TransformerWithCTC(nn.Module):\n",
    "    def __init__(self, d_model, nhead, num_encoder_layers, num_decoder_layers, num_classes):\n",
    "        super(TransformerWithCTC, self).__init__()\n",
    "        self.d_model = d_model\n",
    "\n",
    "        # Image-to-Character Module\n",
    "        self.cnn = ResNetFeatureExtractor(d_model)\n",
    "        \n",
    "        self.pos_encoder = PositionalEncoding2D(d_model, height=256, width=256)  # Assuming 16x16 feature maps\n",
    "        \n",
    "        # self.transformer_encoder = nn.TransformerEncoder(\n",
    "        #     nn.TransformerEncoderLayer(d_model=d_model, nhead=nhead),\n",
    "        #     num_layers=num_encoder_layers\n",
    "        # )\n",
    "\n",
    "        # # Transformer Decoder for C2W\n",
    "        # self.char_embeddings = nn.Embedding(num_classes, d_model)\n",
    "        self.transformer_decoder = nn.TransformerDecoder(\n",
    "            nn.TransformerDecoderLayer(d_model=d_model, nhead=nhead),\n",
    "            num_layers=num_decoder_layers\n",
    "        )\n",
    "\n",
    "        # Transformer for I2C\n",
    "        self.transformer = nn.Transformer(\n",
    "            d_model=d_model,\n",
    "            nhead=nhead,\n",
    "            num_encoder_layers=num_encoder_layers,\n",
    "            num_decoder_layers=num_decoder_layers,\n",
    "            batch_first=True\n",
    "        )\n",
    "\n",
    "        # Character Embeddings as learnable parameters  \n",
    "        self.char_embeddings = nn.Embedding(num_classes, d_model)\n",
    "\n",
    "        # Output heads\n",
    "        self.fc_class = nn.Linear(d_model, num_classes)  # Predict character class\n",
    "        self.fc_position = nn.Linear(d_model, 2)        # Predict character position (x, y) \n",
    "\n",
    "        # CTC Decoder Head\n",
    "        # self.fc_ctc = nn.Linear(d_model, num_classes)\n",
    "\n",
    "    def forward(self, x, tgt_seq):\n",
    "        # CNN Feature extraction\n",
    "        cnn_out = self.cnn(x)                      # Shape: [B, d_model, H, W]\n",
    "        cnn_out = cnn_out.flatten(2).permute(2, 0, 1)  # Reshape to [HW, B, d_model]\n",
    "        print(\"Feature extracted. Size: \", cnn_out.shape)\n",
    "        \n",
    "        cnn_out = self.pos_encoder(cnn_out)        # Add 2D positional encoding\n",
    "        print(\"Positional encoding. Size: \", cnn_out.shape)\n",
    "\n",
    "        # Character Embeddings for the target sequence\n",
    "        # tgt_embeddings = self.char_embeddings(tgt_seq)  # Shape: [T, B, d_model]\n",
    "        # print(\"Character embeddings. Size: \", tgt_embeddings.shape)\n",
    "        \n",
    "        # # Encoder\n",
    "        # memory = self.transformer_encoder(cnn_out)  # Shape: [HW, B, d_model\n",
    "        # print(\"Encoder. Size: \", memory.shape)\n",
    "\n",
    "        # # Decoder\n",
    "        # tgt_embeddings = self.char_embeddings(tgt_seq)  # Shape: [T, B, d_model]\n",
    "        # print(\"Character embeddings. Size: \", tgt_embeddings.shape)\n",
    "\n",
    "\n",
    "        # return ctc_out\n",
    "        # Transformer Encoder-Decoder\n",
    "        transformer_out = self.transformer(cnn_out, tgt_seq)\n",
    "        print(\"Transformer out. Size: \", transformer_out.shape)\n",
    "\n",
    "        # Output: Class and Position predictions\n",
    "        class_out = self.fc_class(transformer_out)      # Shape: [T, B, num_classes]\n",
    "        position_out = self.fc_position(transformer_out)  # Shape: [T, B, 2]\n",
    "\n",
    "        decoded_output = self.transformer_decoder(tgt_embeddings, memory)\n",
    "        print(\"Decoder. Size: \", decoded_output.shape)\n",
    "        # # CTC Head\n",
    "        ctc_out = self.fc_ctc(decoded_output)  # Shape: [T, B, num_classes]\n",
    "        # return class_out, position_out\n",
    "        return ctc_out, class_out, position_out\n",
    "\n",
    "# 4. CTC Decoder\n",
    "def ctc_decoder(ctc_output, blank_label=0):\n",
    "    \"\"\"\n",
    "    Simulates the CTC decoding process.\n",
    "    Args:\n",
    "        ctc_output: Tensor of shape [T, B, num_classes] (logits).\n",
    "        blank_label: The index of the CTC blank label.\n",
    "    Returns:\n",
    "        Decoded output as a list of sequences.\n",
    "    \"\"\"\n",
    "    decoded_sequences = []\n",
    "    for batch in ctc_output.permute(1, 0, 2):  # Iterate over batch dimension\n",
    "        seq = []\n",
    "        prev_token = blank_label\n",
    "        for timestep in batch:\n",
    "            token = timestep.argmax().item()\n",
    "            if token != blank_label and token != prev_token:\n",
    "                seq.append(token)\n",
    "            prev_token = token\n",
    "        decoded_sequences.append(seq)\n",
    "    return decoded_sequences\n",
    "\n",
    "\n",
    "def encode_labels(labels, max_length=None):\n",
    "    \"\"\"\n",
    "    Convert string labels to numerical tensor format.\n",
    "    \n",
    "    Args:\n",
    "        labels (list of str): List of string labels to encode.\n",
    "        max_length (int, optional): Maximum length for padding. If None, it will be determined from the labels.\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: A tensor of shape (batch_size, max_length) containing the encoded labels.\n",
    "    \"\"\"\n",
    "    # Create a mapping from characters to indices\n",
    "    char_to_index = {char: idx + 1 for idx, char in enumerate(sorted(set('abcdefghijklmnopqrstuvwxyz0123456789 ')))}\n",
    "    char_to_index['<blank>'] = 0  # Add a blank token for CTC loss\n",
    "\n",
    "    # Encode the labels\n",
    "    encoded_labels = []\n",
    "    for label in labels:\n",
    "        encoded_label = [char_to_index[char] for char in label if char in char_to_index]\n",
    "        encoded_labels.append(encoded_label)\n",
    "\n",
    "    # Determine the maximum length if not provided\n",
    "    if max_length is None:\n",
    "        max_length = max(len(seq) for seq in encoded_labels)\n",
    "\n",
    "    # Pad sequences to the maximum length\n",
    "    padded_labels = [seq + [0] * (max_length - len(seq)) for seq in encoded_labels]  # 0 for padding\n",
    "\n",
    "    return torch.tensor(padded_labels, dtype=torch.long)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data.dataloader import SCUTLoader\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "import torchvision.transforms as transforms\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Unnamed: 0 img_file_name           label\n",
      "0           0     00000.jpg   MEXICO, D. F.\n",
      "1           0     00001.jpg  Tel.5-45-25-05\n",
      "2           0     00002.jpg  Newton No. 136\n",
      "3           0     00003.jpg          CEDROS\n",
      "4           0     00004.jpg      PASTELERIA\n",
      "Dataset size:  7703\n",
      "Input images shape: torch.Size([32, 3, 128, 128])\n",
      "Target sequence shape: torch.Size([32, 31])\n",
      "Batch index:  0\n",
      "Input images:  tensor([[[1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         ...,\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.]],\n",
      "\n",
      "        [[1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         ...,\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.]],\n",
      "\n",
      "        [[1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         ...,\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.]]])\n",
      "Target sequence:  tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0])\n",
      "Feature extracted. Size:  torch.Size([16, 32, 512])\n",
      "Positional encoding. Size:  torch.Size([16, 32, 512])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "the batch number of src and tgt must be equal",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 75\u001b[0m\n\u001b[0;32m     72\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInput images: \u001b[39m\u001b[38;5;124m\"\u001b[39m, input_images[\u001b[38;5;241m0\u001b[39m])\n\u001b[0;32m     73\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTarget sequence: \u001b[39m\u001b[38;5;124m\"\u001b[39m, tgt_seq[\u001b[38;5;241m0\u001b[39m])\n\u001b[1;32m---> 75\u001b[0m predicted_word, class_out, position_outs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_images\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtgt_seq\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Assuming model takes only input_images\u001b[39;00m\n\u001b[0;32m     76\u001b[0m \u001b[38;5;28mprint\u001b[39m(class_out)\n\u001b[0;32m     77\u001b[0m \u001b[38;5;28mprint\u001b[39m(position_out)\n",
      "File \u001b[1;32md:\\Program Files\\Python\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\Program Files\\Python\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[1], line 115\u001b[0m, in \u001b[0;36mTransformerWithCTC.forward\u001b[1;34m(self, x, tgt_seq)\u001b[0m\n\u001b[0;32m     98\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPositional encoding. Size: \u001b[39m\u001b[38;5;124m\"\u001b[39m, cnn_out\u001b[38;5;241m.\u001b[39mshape)\n\u001b[0;32m    100\u001b[0m \u001b[38;5;66;03m# Character Embeddings for the target sequence\u001b[39;00m\n\u001b[0;32m    101\u001b[0m \u001b[38;5;66;03m# tgt_embeddings = self.char_embeddings(tgt_seq)  # Shape: [T, B, d_model]\u001b[39;00m\n\u001b[0;32m    102\u001b[0m \u001b[38;5;66;03m# print(\"Character embeddings. Size: \", tgt_embeddings.shape)\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    113\u001b[0m \u001b[38;5;66;03m# return ctc_out\u001b[39;00m\n\u001b[0;32m    114\u001b[0m \u001b[38;5;66;03m# Transformer Encoder-Decoder\u001b[39;00m\n\u001b[1;32m--> 115\u001b[0m transformer_out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransformer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcnn_out\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtgt_seq\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    116\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTransformer out. Size: \u001b[39m\u001b[38;5;124m\"\u001b[39m, transformer_out\u001b[38;5;241m.\u001b[39mshape)\n\u001b[0;32m    118\u001b[0m \u001b[38;5;66;03m# Output: Class and Position predictions\u001b[39;00m\n",
      "File \u001b[1;32md:\\Program Files\\Python\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\Program Files\\Python\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32md:\\Program Files\\Python\\Lib\\site-packages\\torch\\nn\\modules\\transformer.py:213\u001b[0m, in \u001b[0;36mTransformer.forward\u001b[1;34m(self, src, tgt, src_mask, tgt_mask, memory_mask, src_key_padding_mask, tgt_key_padding_mask, memory_key_padding_mask, src_is_causal, tgt_is_causal, memory_is_causal)\u001b[0m\n\u001b[0;32m    211\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mthe batch number of src and tgt must be equal\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    212\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_first \u001b[38;5;129;01mand\u001b[39;00m src\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m) \u001b[38;5;241m!=\u001b[39m tgt\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m is_batched:\n\u001b[1;32m--> 213\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mthe batch number of src and tgt must be equal\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    215\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m src\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39md_model \u001b[38;5;129;01mor\u001b[39;00m tgt\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39md_model:\n\u001b[0;32m    216\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mthe feature number of src and tgt must be equal to d_model\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: the batch number of src and tgt must be equal"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "\n",
    "# Assuming TransformerWithCTC and SCUTLoader are defined elsewhere\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Hyperparameters\n",
    "    BATCH_SIZE = 32\n",
    "    IMAGE_SIZE = (3, 224, 224)\n",
    "    D_MODEL = 512\n",
    "    NHEAD = 8\n",
    "    NUM_ENCODER_LAYERS = 3\n",
    "    NUM_DECODER_LAYERS = 1\n",
    "    NUM_CLASSES = 37  # 26 letters + 1 blank token + 0-9\n",
    "    TGT_SEQ_LEN = 10   # Target sequence length\n",
    "\n",
    "    DATA_IMG_PATH = \"./data/SCUT-CTW1500/cropped_train_images2/\"\n",
    "    DATA_LABELS_PATH = \"./data/SCUT-CTW1500/processed_labels2.csv\"\n",
    "\n",
    "    # Model\n",
    "    model = TransformerWithCTC(\n",
    "        d_model=D_MODEL,\n",
    "        nhead=NHEAD,\n",
    "        num_encoder_layers=NUM_ENCODER_LAYERS,\n",
    "        num_decoder_layers=NUM_DECODER_LAYERS,\n",
    "        num_classes=NUM_CLASSES\n",
    "    )\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((128, 128)),  # Resize images to 256x256\n",
    "        transforms.ToTensor(),  # Convert image to PyTorch tensor\n",
    "        # transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  # Normalize based on ImageNet stats\n",
    "    ])\n",
    "\n",
    "    # Load dataset\n",
    "    dataset = SCUTLoader(image_dir=DATA_IMG_PATH, label_dir=DATA_LABELS_PATH, transform=transform)\n",
    "    print(\"Dataset size: \", len(dataset))\n",
    "    \n",
    "    train_size = int(0.8 * len(dataset))\n",
    "    test_size = len(dataset) - train_size\n",
    "    train_dataset, test_dataset = random_split(dataset, [train_size, test_size])\n",
    "    \n",
    "    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=0)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=0)\n",
    "\n",
    "    # Move model to device\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "\n",
    "    # Define loss function and optimizer\n",
    "    criterion = nn.CTCLoss(blank=37)  # Assuming 37 is the index for the blank token\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "    # Training loop\n",
    "    num_epochs = 10\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        for batch_idx, (input_images, tgt_seq) in enumerate(train_loader):\n",
    "            # input_images, tgt_seq = input_images.to(device), tgt_seq.to(device)\n",
    "            input_images = input_images.to(device)\n",
    "            tgt_seq = encode_labels(tgt_seq)\n",
    "             \n",
    "            tgt_seq = tgt_seq.to(device)\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Forward pass\n",
    "            print(\"Input images shape:\", input_images.shape)\n",
    "            print(\"Target sequence shape:\", tgt_seq.shape)\n",
    "\n",
    "            print(\"Batch index: \", batch_idx)\n",
    "            print(\"Input images: \", input_images[0])\n",
    "            print(\"Target sequence: \", tgt_seq[0])\n",
    "\n",
    "            predicted_word, class_out, position_outs = model(input_images, tgt_seq)  # Assuming model takes only input_images\n",
    "            print(class_out)\n",
    "            print(position_out)\n",
    "            decoded_sequences = ctc_decoder(predicted_word, blank_label=0)\n",
    "            print(decoded_sequences)\n",
    "\n",
    "            # Calculate CTC loss\n",
    "            # Note: You may need to adjust the shape of ctc_out and tgt_seq\n",
    "            # Ensure tgt_seq is the correct shape for CTC loss\n",
    "            loss = criterion(ctc_out, tgt_seq)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            if batch_idx % 10 == 0:\n",
    "                print(f'Epoch [{epoch+1}/{num_epochs}], Step [{batch_idx}/{len(train_loader)}], Loss: {loss.item():.4f}')\n",
    "\n",
    "    # After training, you can evaluate the model on the test set\n",
    "    # model.eval()\n",
    "    # with torch.no_grad():\n",
    "        # for input_images, tgt_seq in test_loader:\n",
    "            # input_images = input_images.to(device)\n",
    "            # ctc_out = model(input_images)\n",
    "            # Decode the output\n",
    "            # decoded_sequences = ctc_decoder(ctc_out, blank_label=0)\n",
    "            # Process decoded_sequences as needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "# torch.cuda.is_available()\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
